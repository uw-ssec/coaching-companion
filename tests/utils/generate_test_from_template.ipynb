{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Unit Tests for SQLModels from Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /workspaces/coaching-companion/tests/models\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Remove the last folder from the current working directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = os.path.join(parent_dir, \"models\")\n",
    "\n",
    "# Try and except block to check if the folder exists\n",
    "try:\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created directory: {output_folder}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {output_folder}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Models to Generate Unit Tests\n",
    "All the models will be based on the BaseTableModel, and you will need to customize the tests for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Read the model data from the yaml file\n",
    "def load_yaml(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWSDashboard', 'Access', 'AccreditationCertificate', 'ActionPlans', 'AddPlayByPlay', 'Alias', 'Annotations', 'AssessmentsClassroom', 'AssessmentsDashboard', 'AssessmentsEvaluation', 'AssessmentsLister', 'AssessmentsRating', 'AssessmentsReport', 'AssessmentsReportClassroom', 'AssessmentsSite', 'Assets', 'Assignments', 'AuthUser', 'BaseTableModel', 'CoachingPartnership', 'CollaboraEndpoint', 'Comments', 'ConnectToMerit', 'ContactForm', 'CopAssets', 'CopGroups', 'CopLocation', 'CoursesState', 'CycleDashboard', 'DiscussionBoards', 'DiscussionTopics', 'Documentation', 'DualLanguageHandbook', 'DualLanguageSubmission', 'EaEvalSurvey', 'Endpoint', 'FamilySurveyLink', 'Grade', 'Inquisitor', 'InstructorMaterials', 'Item', 'Layout', 'Limit', 'Links', 'Lister', 'Location', 'Login', 'Logout', 'LtiEndpoint', 'ManageComponents', 'ManageDatalists', 'ManageGroups', 'ManageMenus', 'ManageRecipes', 'ManageSite', 'ManageStudents', 'ManageUsers', 'Media', 'MediaDatabase', 'MeritEndpoint', 'MssqlServer', 'Note', 'ProgramProfileSubmission', 'RatingsRubric', 'RecordsReviewHandbook', 'RecordsReviewSubmission', 'ReflectionAssignment', 'Reports', 'ReportsBuilder', 'ResetPassword', 'ResourceLibrary', 'SharedAssets', 'SharedGoals', 'Solotron', 'StudentSubmissions', 'StudentSubmissionsSlug', 'SupportingDocumentation', 'Tagit', 'Tracker', 'UnassignedAssets', 'UploadAndReflectionAssignment', 'UserInvitations', 'UserLogAllAttributes', 'UserLogCompletedGoals', 'UserLogCopGroups', 'UserLogCopGroupsDigest', 'UserLogCreatedAt', 'UserLogPartnerships', 'UserLogProperties', 'UserLogReporting', 'UserLogSharedGoals', 'UserLogUploads', 'UserSettings', 'UserSubmissions', 'VideoConference', 'VideoHighlightSubmission', 'VideoReflection']\n"
     ]
    }
   ],
   "source": [
    "# Load the model data\n",
    "model_data = load_yaml(os.path.join(os.getcwd(), \"models_2_template2.yml\"))\n",
    "print(model_data[\"models\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model and the Test Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Open a file and write the model template\n",
    "def write_model_template(output_folder, model_name, model_template):\n",
    "    output_filepath = os.path.join(output_folder,f\"test_{model_name}.py\")\n",
    "    with open(output_filepath, \"w\") as file:\n",
    "        file.write(f\"{model_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_data[\"models\"]:\n",
    "    model_template = \\\n",
    "f\"\"\"import pytest\n",
    "from datetime import datetime, timezone\n",
    "from coaching_companion.models import {model}\n",
    "\n",
    "# Define the datetime string\n",
    "datetime_str = \"2021-12-01T00:00:00Z\"\n",
    "# Parse the datetime string into a datetime object\n",
    "datetime_obj = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "# Convert the datetime object to a Unix timestamp (float)\n",
    "unix_timestamp = datetime_obj.replace(tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "# Define the url string\n",
    "url_str = \"https://s3-us-west-2.amazonaws.com/test-bucket/test-key\"\n",
    "\n",
    "# Define the UUID string\n",
    "uuid_int = 12345678901234567890\n",
    "\n",
    "@pytest.mark.parametrize(\"unix_timestamp\", [unix_timestamp]) # Allows us to define a single test with multiple potential inputs\n",
    "def test_{model.lower()}(unix_timestamp):\n",
    "    # Create an instance of {model}\n",
    "    dashboard = {model}.model_validate(\n",
    "        {{\"created_at\": unix_timestamp,\n",
    "        \"created_by\": int(12345678901234567890),\n",
    "        \"title\": \"Test {model} Title\",\n",
    "        \"type_\": \"{model.lower()}\"}}\n",
    "    )\n",
    "\n",
    "    # Assert that the fields are correctly set\n",
    "    assert dashboard.created_by == int(12345678901234567890)\n",
    "    assert dashboard.title == \"Test {model} Title\"\n",
    "    assert dashboard.type_ == \"{model.lower()}\"\n",
    "\n",
    "    # Convert the Unix timestamp to a UTC datetime object\n",
    "    expected_created_at = datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)\n",
    "    # Format the datetime object to the desired string format\n",
    "    expected_created_at_str = expected_created_at.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # Assert that the created_at field is correctly converted and formatted\n",
    "    assert expected_created_at_str == datetime_str\n",
    "\n",
    "def test_{model.lower()}_default_values():\n",
    "    # Create an instance of {model} without optional fields\n",
    "    dashboard = {model}(title=\"Test Dashboard\")\n",
    "\n",
    "    # Assert that the default values are correctly set\n",
    "    assert dashboard.id is None\n",
    "    assert dashboard.created_by is None\n",
    "    assert dashboard.created_at is None\n",
    "\n",
    "def test_{model.lower()}_name_max_length():\n",
    "    # Create an instance of {model} with a name exceeding max_length\n",
    "    long_name = \"A\" * 300 # For testing non-text type fields\n",
    "    long_num = 12345678901234567890 # For testing text type fields\n",
    "    # Validate the id\n",
    "    with pytest.raises(ValueError):\n",
    "        {model}.model_validate({{\"id\": long_name}})\n",
    "    # Validate the title\n",
    "    with pytest.raises(ValueError):\n",
    "        {model}.model_validate({{\"title\": long_name}})\n",
    "    # Validate the created_by\n",
    "    with pytest.raises(ValueError):\n",
    "        {model}.model_validate({{\"created_by\": long_name}})\n",
    "    # Validate the created_at\n",
    "    with pytest.raises(OverflowError):\n",
    "        {model}.model_validate({{\"created_at\": long_num}})\n",
    "    # Validate the created_at\n",
    "    with pytest.raises(TypeError):\n",
    "        {model}.model_validate({{\"created_at\": long_name}})\n",
    "    # Validate the type_\n",
    "    with pytest.raises(ValueError):\n",
    "        {model}.model_validate({{\"type_\": long_name}})\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main()\n",
    "    \n",
    "# References:\n",
    "# - https://github.com/fastapi/sqlmodel/issues/52\n",
    "# - https://www.datacamp.com/tutorial/pytest-tutorial-a-hands-on-guide-to-unit-testing\n",
    "    \"\"\"\n",
    "    \n",
    "    write_model_template(output_folder=output_folder, model_name=model.lower(), model_template=model_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
